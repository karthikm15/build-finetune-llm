{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i29BVVlUdbpQ"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IXGJBoF8HJz",
        "outputId": "f9f1bcce-2c06-4050-e02b-a872754855d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Nov 28 03:19:37 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    26W /  70W |  13923MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENFSqOnlQsB6",
        "outputId": "cab311fc-b245-4950-c936-05d7d770033f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb --quiet\n",
        "!pip install torchsummaryX -q\n",
        "!pip install datasets\n",
        "!pip install zstandard\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhOGy0oVQR2z",
        "outputId": "9417a160-520e-4264-ba47-b642867823e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import zstandard\n",
        "import datasets\n",
        "import tiktoken\n",
        "import random\n",
        "import wandb\n",
        "import math\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa2w5xXiU6OV"
      },
      "outputs": [],
      "source": [
        "### If you are using colab, you can import google drive to save model checkpoints in a folder.\n",
        "### This is used when connecting to GCE VMs, but the user still wants to connect to Google Drive\n",
        "import os.path as path\n",
        "if not path.exists(\"/content/drive\"):\n",
        "  !sudo add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "  !sudo apt-get update -qq 2>&1 > /dev/null\n",
        "  !sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null\n",
        "  !google-drive-ocamlfuse\n",
        "\n",
        "  !sudo apt-get install -qq w3m # to act as web browser\n",
        "  !xdg-settings set default-web-browser w3m.desktop # to set default browser\n",
        "  %cd /content\n",
        "  !mkdir drive\n",
        "  %cd drive\n",
        "  !mkdir MyDrive\n",
        "  %cd ..\n",
        "  %cd ..\n",
        "  !google-drive-ocamlfuse /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSotppG01ltB"
      },
      "source": [
        "# Download OpenWebText Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TI3NFBuGQvpQ"
      },
      "outputs": [],
      "source": [
        "# 13GB dataset: https://huggingface.co/datasets/Skylion007/openwebtext\n",
        "# Small dataset: stas/openwebtext-10k\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"kerpr/cc_openwebtext\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGzNjQzNsqN7"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'epochs'        : 5,\n",
        "    'batch_size'    : 32,\n",
        "    'init_lr'       : 3e-5,\n",
        "    'block_size'    : 256,\n",
        "    'dropout'       : 0.1,\n",
        "    'vocab_size'    : 50257,\n",
        "    'bias'          : True,\n",
        "    'n_layer'       : 12,\n",
        "    'n_head'        : 10,\n",
        "    'n_embd'        : 250,\n",
        "    'end_token'     : 50256\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKZJr23a7CSf"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZytDJnKBdZ6f"
      },
      "source": [
        "## Dataloader / Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps2lMYSQbryZ"
      },
      "outputs": [],
      "source": [
        "data = dataset[\"train\"].train_test_split(test_size=0.05, seed=1200, shuffle=True)\n",
        "test_valid = data['test'].train_test_split(test_size=0.5)\n",
        "train_test_valid_dataset = datasets.DatasetDict({\n",
        "    'train': data['train'],\n",
        "    'test': test_valid['test'],\n",
        "    'valid': test_valid['train']})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZthtJEsmCz-"
      },
      "outputs": [],
      "source": [
        "# Dataset class to load train and validation data\n",
        "\n",
        "class OpenWebTextDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, prefix):\n",
        "\n",
        "        prev_data = train_test_valid_dataset[prefix]\n",
        "        enc = tiktoken.get_encoding(\"gpt2\") # encoding using the tiktoken library\n",
        "        self.values = [enc.encode_ordinary(prev_data[i][\"text\"]) if (prev_data[i][\"text\"] != None) else [] for i in range(len(prev_data))]\n",
        "\n",
        "        # Append start and end token\n",
        "        self.values = [np.array(self.values[i] + [config['end_token']]) for i in range(len(self.values))]\n",
        "        self.values = np.concatenate(self.values, axis=0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.values) // config['block_size']\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "        # Shift x and y by one and then index by block_size amount of tokens\n",
        "        x = torch.from_numpy(self.values[(ind*config['block_size']):(ind*config['block_size'])+config['block_size']].astype(np.int64))\n",
        "        y = torch.from_numpy(self.values[(ind*config['block_size'] + 1):(ind*config['block_size'])+config['block_size']+1].astype(np.int64))\n",
        "\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cw2dCUHtCyD"
      },
      "outputs": [],
      "source": [
        "class OpenWebTextTestDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, prefix):\n",
        "\n",
        "      prev_data = train_test_valid_dataset[\"test\"]\n",
        "      enc = tiktoken.get_encoding(\"gpt2\") # encoding using the tiktoken library\n",
        "      self.values = [enc.encode_ordinary(prev_data[i][\"text\"]) if (prev_data[i][\"text\"] != None) else [] for i in range(len(prev_data))]\n",
        "\n",
        "      self.values = [np.array(self.values[i] + [config['end_token']]) for i in range(len(self.values))] #start\n",
        "      self.values = np.concatenate(self.values, axis=0)\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.values) // config['block_size']\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "      x = torch.from_numpy(self.values[(ind*config['block_size']):(ind*config['block_size'])+config['block_size']].astype(np.int64))\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77P7oMCGmwnC"
      },
      "outputs": [],
      "source": [
        "train_data = OpenWebTextDataset(prefix=\"train\")\n",
        "val_data = OpenWebTextDataset(prefix=\"valid\")\n",
        "test_data = OpenWebTextTestDataset(prefix=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riHuUjIrmxSd"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data,\n",
        "    num_workers = 1,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = True\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data,\n",
        "    num_workers = 1,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_data,\n",
        "    num_workers = 1,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibZgc9kMt_LN",
        "outputId": "39d0aca8-45b9-415a-d3f6-1a115ced3115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 256]) torch.Size([32, 256])\n",
            "tensor([[  513,    13,    15,  ...,  2810,   262,   636],\n",
            "        [14196,   526,   198,  ...,   257,   471,    13],\n",
            "        [  447,   247,   303,  ...,    12,    33,    12],\n",
            "        ...,\n",
            "        [13742,   357,    16,  ...,   262,  2520,  2807],\n",
            "        [13449,   532,  3267,  ...,    12,    23,  3648],\n",
            "        [ 1919,  2324,  1271,  ...,  4179,   534,  7111]]) tensor([[   13,    15, 33721,  ...,   262,   636,   468],\n",
            "        [  526,   198,   198,  ...,   471,    13,    50],\n",
            "        [  247,   303,  1775,  ...,    33,    12,    34],\n",
            "        ...,\n",
            "        [  357,    16,     8,  ...,  2520,  2807,  6896],\n",
            "        [  532,  3267,   860,  ...,    23,  3648,  9595],\n",
            "        [ 2324,  1271,  1906,  ...,   534,  7111,    11]])\n",
            "torch.Size([32, 256])\n",
            "tensor([[15592,  1578,   402,  ...,    14,   940,    14],\n",
            "        [ 1238,  1828,   860,  ...,  7841,  1008,    13],\n",
            "        [10366,   654,    13,  ..., 28682,   329,   691],\n",
            "        ...,\n",
            "        [  481,  2148,   262,  ...,    60, 11709, 22935],\n",
            "        [  820,    58,    15,  ...,    13,  4906, 11709],\n",
            "        [22935,  1416,    79,  ...,   263, 31815,    58]])\n"
          ]
        }
      ],
      "source": [
        "# Testing code to check if your data loaders are working\n",
        "for i, data in enumerate(train_loader):\n",
        "    x, y = data\n",
        "    print(x.shape, y.shape)\n",
        "    print(x, y)\n",
        "    break\n",
        "\n",
        "for i, data in enumerate(test_loader):\n",
        "    x = data\n",
        "    print(x.shape)\n",
        "    print(x)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNqW_6a70QBm"
      },
      "source": [
        "## Blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyKpV5Dj0Vcp"
      },
      "outputs": [],
      "source": [
        "# Layer normalization for regularizing the model\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, ndim, bias):\n",
        "    super().__init__()\n",
        "    self.weight, self.bias = nn.Parameter(torch.ones(ndim)), nn.Parameter(torch.zeros(ndim))\n",
        "\n",
        "  def forward(self, input):\n",
        "    return nn.functional.layer_norm(input=input, \n",
        "                                    normalized_shape=self.weight.shape, \n",
        "                                    weight=self.weight, \n",
        "                                    bias=self.bias, \n",
        "                                    eps=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBFxU79i1aoz"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.attention_layer = nn.Linear(config['n_embd'], 3 * config['n_embd'])\n",
        "    self.projection_layer = nn.Linear(config['n_embd'], config['n_embd'])\n",
        "    self.dropout = nn.Dropout(config['dropout'])\n",
        "\n",
        "  def attention_calculation (self, x):\n",
        "    query, key, value = self.attention_layer(x).split(config['n_embd'], dim=2)\n",
        "    key = key.view(x.size(0), x.size(1), config['n_head'], x.size(2) // config['n_head'])\n",
        "    query = query.view(x.size(0), x.size(1), config['n_head'], x.size(2) // config['n_head'])\n",
        "    value = value.view(x.size(0), x.size(1), config['n_head'], x.size(2) // config['n_head'])\n",
        "\n",
        "    key, query, value = key.transpose(1, 2), query.transpose(1, 2), value.transpose(1, 2)\n",
        "\n",
        "    y = nn.functional.scaled_dot_product_attention(query, key, value, dropout_p=config['dropout'])\n",
        "\n",
        "  def forward(self, x):\n",
        "    y = self.attention_calculation(x)\n",
        "    out = y.transpose(1, 2).view(x.size(0), x.size(1), x.size(2))\n",
        "    out = self.projection_layer(out)\n",
        "    out = self.dropout(out)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8m-ZmX4G7uWt"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config['n_embd'], bias=config['bias'])\n",
        "        self.attn = AttentionLayer()\n",
        "        self.ln_2 = LayerNorm(config['n_embd'], bias=config['bias'])\n",
        "        self.mlp = nn.Sequential (\n",
        "            nn.Linear(config['n_embd'], 4 * config['n_embd']),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config['n_embd'], config['n_embd']),\n",
        "            nn.Dropout(config['dropout'])\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ws_ajko8_U6"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeN0qJhG9BXj"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embed = nn.Embedding(config['vocab_size'], config['n_embd'])\n",
        "    self.pos_embed = nn.Embedding(config['block_size'], config['n_embd'])\n",
        "    self.dropout = nn.Dropout(config['dropout'])\n",
        "\n",
        "    modules = [Block() for b in range(config['n_layer'])]\n",
        "    self.blocks = nn.Sequential(*modules)\n",
        "    self.layernorm = LayerNorm(config['n_embd'], bias=config['bias'])\n",
        "\n",
        "    self.lin1 = nn.Linear(config['n_embd'], config['vocab_size'])\n",
        "    self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "  def forward(self, idx):\n",
        "      position = torch.arange(0, idx.size(1))\n",
        "\n",
        "      tok_emb = self.token_embed(idx)\n",
        "      pos_emb = self.pos_embed(position)\n",
        "      x = self.dropout(tok_emb + pos_emb)\n",
        "\n",
        "      for block in self.blocks: x = block(x)\n",
        "      x = self.layernorm(x)\n",
        "\n",
        "      return self.lin1(x)\n",
        "\n",
        "  def generate(self, idx, max_new_tokens, temperature=1.0):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx if idx.size(1) <= self.config['block_size'] else idx[:, -self.config['block_size']:]\n",
        "        logits, _ = self(idx_cond)\n",
        "        probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UyWlj1F0KTpd",
        "outputId": "43fe8714-18a7-4eeb-e472-6be096dce69f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=======================================================================================================\n",
            "                                                 Kernel Shape  \\\n",
            "Layer                                                           \n",
            "0_transformer.Embedding_wte                      [250, 50257]   \n",
            "1_transformer.Embedding_wpe                        [250, 256]   \n",
            "2_transformer.Dropout_drop                                  -   \n",
            "3_transformer.h.0.LayerNorm_ln_1                        [250]   \n",
            "4_transformer.h.0.attn.Linear_c_attn               [250, 750]   \n",
            "5_transformer.h.0.attn.Linear_c_proj               [250, 250]   \n",
            "6_transformer.h.0.attn.Dropout_resid_dropout                -   \n",
            "7_transformer.h.0.LayerNorm_ln_2                        [250]   \n",
            "8_transformer.h.0.mlp.Linear_c_fc                 [250, 1000]   \n",
            "9_transformer.h.0.mlp.GELU_gelu                             -   \n",
            "10_transformer.h.0.mlp.Linear_c_proj              [1000, 250]   \n",
            "11_transformer.h.0.mlp.Dropout_dropout                      -   \n",
            "12_transformer.h.1.LayerNorm_ln_1                       [250]   \n",
            "13_transformer.h.1.attn.Linear_c_attn              [250, 750]   \n",
            "14_transformer.h.1.attn.Linear_c_proj              [250, 250]   \n",
            "15_transformer.h.1.attn.Dropout_resid_dropout               -   \n",
            "16_transformer.h.1.LayerNorm_ln_2                       [250]   \n",
            "17_transformer.h.1.mlp.Linear_c_fc                [250, 1000]   \n",
            "18_transformer.h.1.mlp.GELU_gelu                            -   \n",
            "19_transformer.h.1.mlp.Linear_c_proj              [1000, 250]   \n",
            "20_transformer.h.1.mlp.Dropout_dropout                      -   \n",
            "21_transformer.h.2.LayerNorm_ln_1                       [250]   \n",
            "22_transformer.h.2.attn.Linear_c_attn              [250, 750]   \n",
            "23_transformer.h.2.attn.Linear_c_proj              [250, 250]   \n",
            "24_transformer.h.2.attn.Dropout_resid_dropout               -   \n",
            "25_transformer.h.2.LayerNorm_ln_2                       [250]   \n",
            "26_transformer.h.2.mlp.Linear_c_fc                [250, 1000]   \n",
            "27_transformer.h.2.mlp.GELU_gelu                            -   \n",
            "28_transformer.h.2.mlp.Linear_c_proj              [1000, 250]   \n",
            "29_transformer.h.2.mlp.Dropout_dropout                      -   \n",
            "30_transformer.h.3.LayerNorm_ln_1                       [250]   \n",
            "31_transformer.h.3.attn.Linear_c_attn              [250, 750]   \n",
            "32_transformer.h.3.attn.Linear_c_proj              [250, 250]   \n",
            "33_transformer.h.3.attn.Dropout_resid_dropout               -   \n",
            "34_transformer.h.3.LayerNorm_ln_2                       [250]   \n",
            "35_transformer.h.3.mlp.Linear_c_fc                [250, 1000]   \n",
            "36_transformer.h.3.mlp.GELU_gelu                            -   \n",
            "37_transformer.h.3.mlp.Linear_c_proj              [1000, 250]   \n",
            "38_transformer.h.3.mlp.Dropout_dropout                      -   \n",
            "39_transformer.h.4.LayerNorm_ln_1                       [250]   \n",
            "40_transformer.h.4.attn.Linear_c_attn              [250, 750]   \n",
            "41_transformer.h.4.attn.Linear_c_proj              [250, 250]   \n",
            "42_transformer.h.4.attn.Dropout_resid_dropout               -   \n",
            "43_transformer.h.4.LayerNorm_ln_2                       [250]   \n",
            "44_transformer.h.4.mlp.Linear_c_fc                [250, 1000]   \n",
            "45_transformer.h.4.mlp.GELU_gelu                            -   \n",
            "46_transformer.h.4.mlp.Linear_c_proj              [1000, 250]   \n",
            "47_transformer.h.4.mlp.Dropout_dropout                      -   \n",
            "48_transformer.h.5.LayerNorm_ln_1                       [250]   \n",
            "49_transformer.h.5.attn.Linear_c_attn              [250, 750]   \n",
            "50_transformer.h.5.attn.Linear_c_proj              [250, 250]   \n",
            "51_transformer.h.5.attn.Dropout_resid_dropout               -   \n",
            "52_transformer.h.5.LayerNorm_ln_2                       [250]   \n",
            "53_transformer.h.5.mlp.Linear_c_fc                [250, 1000]   \n",
            "54_transformer.h.5.mlp.GELU_gelu                            -   \n",
            "55_transformer.h.5.mlp.Linear_c_proj              [1000, 250]   \n",
            "56_transformer.h.5.mlp.Dropout_dropout                      -   \n",
            "57_transformer.h.6.LayerNorm_ln_1                       [250]   \n",
            "58_transformer.h.6.attn.Linear_c_attn              [250, 750]   \n",
            "59_transformer.h.6.attn.Linear_c_proj              [250, 250]   \n",
            "60_transformer.h.6.attn.Dropout_resid_dropout               -   \n",
            "61_transformer.h.6.LayerNorm_ln_2                       [250]   \n",
            "62_transformer.h.6.mlp.Linear_c_fc                [250, 1000]   \n",
            "63_transformer.h.6.mlp.GELU_gelu                            -   \n",
            "64_transformer.h.6.mlp.Linear_c_proj              [1000, 250]   \n",
            "65_transformer.h.6.mlp.Dropout_dropout                      -   \n",
            "66_transformer.h.7.LayerNorm_ln_1                       [250]   \n",
            "67_transformer.h.7.attn.Linear_c_attn              [250, 750]   \n",
            "68_transformer.h.7.attn.Linear_c_proj              [250, 250]   \n",
            "69_transformer.h.7.attn.Dropout_resid_dropout               -   \n",
            "70_transformer.h.7.LayerNorm_ln_2                       [250]   \n",
            "71_transformer.h.7.mlp.Linear_c_fc                [250, 1000]   \n",
            "72_transformer.h.7.mlp.GELU_gelu                            -   \n",
            "73_transformer.h.7.mlp.Linear_c_proj              [1000, 250]   \n",
            "74_transformer.h.7.mlp.Dropout_dropout                      -   \n",
            "75_transformer.h.8.LayerNorm_ln_1                       [250]   \n",
            "76_transformer.h.8.attn.Linear_c_attn              [250, 750]   \n",
            "77_transformer.h.8.attn.Linear_c_proj              [250, 250]   \n",
            "78_transformer.h.8.attn.Dropout_resid_dropout               -   \n",
            "79_transformer.h.8.LayerNorm_ln_2                       [250]   \n",
            "80_transformer.h.8.mlp.Linear_c_fc                [250, 1000]   \n",
            "81_transformer.h.8.mlp.GELU_gelu                            -   \n",
            "82_transformer.h.8.mlp.Linear_c_proj              [1000, 250]   \n",
            "83_transformer.h.8.mlp.Dropout_dropout                      -   \n",
            "84_transformer.h.9.LayerNorm_ln_1                       [250]   \n",
            "85_transformer.h.9.attn.Linear_c_attn              [250, 750]   \n",
            "86_transformer.h.9.attn.Linear_c_proj              [250, 250]   \n",
            "87_transformer.h.9.attn.Dropout_resid_dropout               -   \n",
            "88_transformer.h.9.LayerNorm_ln_2                       [250]   \n",
            "89_transformer.h.9.mlp.Linear_c_fc                [250, 1000]   \n",
            "90_transformer.h.9.mlp.GELU_gelu                            -   \n",
            "91_transformer.h.9.mlp.Linear_c_proj              [1000, 250]   \n",
            "92_transformer.h.9.mlp.Dropout_dropout                      -   \n",
            "93_transformer.h.10.LayerNorm_ln_1                      [250]   \n",
            "94_transformer.h.10.attn.Linear_c_attn             [250, 750]   \n",
            "95_transformer.h.10.attn.Linear_c_proj             [250, 250]   \n",
            "96_transformer.h.10.attn.Dropout_resid_dropout              -   \n",
            "97_transformer.h.10.LayerNorm_ln_2                      [250]   \n",
            "98_transformer.h.10.mlp.Linear_c_fc               [250, 1000]   \n",
            "99_transformer.h.10.mlp.GELU_gelu                           -   \n",
            "100_transformer.h.10.mlp.Linear_c_proj            [1000, 250]   \n",
            "101_transformer.h.10.mlp.Dropout_dropout                    -   \n",
            "102_transformer.h.11.LayerNorm_ln_1                     [250]   \n",
            "103_transformer.h.11.attn.Linear_c_attn            [250, 750]   \n",
            "104_transformer.h.11.attn.Linear_c_proj            [250, 250]   \n",
            "105_transformer.h.11.attn.Dropout_resid_dropout             -   \n",
            "106_transformer.h.11.LayerNorm_ln_2                     [250]   \n",
            "107_transformer.h.11.mlp.Linear_c_fc              [250, 1000]   \n",
            "108_transformer.h.11.mlp.GELU_gelu                          -   \n",
            "109_transformer.h.11.mlp.Linear_c_proj            [1000, 250]   \n",
            "110_transformer.h.11.mlp.Dropout_dropout                    -   \n",
            "111_transformer.LayerNorm_ln_f                          [250]   \n",
            "112_lm_head                                      [250, 50257]   \n",
            "\n",
            "                                                     Output Shape     Params  \\\n",
            "Layer                                                                          \n",
            "0_transformer.Embedding_wte                        [32, 256, 250]  12.56425M   \n",
            "1_transformer.Embedding_wpe                            [256, 250]      64.0k   \n",
            "2_transformer.Dropout_drop                         [32, 256, 250]          -   \n",
            "3_transformer.h.0.LayerNorm_ln_1                   [32, 256, 250]      500.0   \n",
            "4_transformer.h.0.attn.Linear_c_attn               [32, 256, 750]    188.25k   \n",
            "5_transformer.h.0.attn.Linear_c_proj               [32, 256, 250]     62.75k   \n",
            "6_transformer.h.0.attn.Dropout_resid_dropout       [32, 256, 250]          -   \n",
            "7_transformer.h.0.LayerNorm_ln_2                   [32, 256, 250]      500.0   \n",
            "8_transformer.h.0.mlp.Linear_c_fc                 [32, 256, 1000]     251.0k   \n",
            "9_transformer.h.0.mlp.GELU_gelu                   [32, 256, 1000]          -   \n",
            "10_transformer.h.0.mlp.Linear_c_proj               [32, 256, 250]    250.25k   \n",
            "11_transformer.h.0.mlp.Dropout_dropout             [32, 256, 250]          -   \n",
            "12_transformer.h.1.LayerNorm_ln_1                  [32, 256, 250]      500.0   \n",
            "13_transformer.h.1.attn.Linear_c_attn              [32, 256, 750]    188.25k   \n",
            "14_transformer.h.1.attn.Linear_c_proj              [32, 256, 250]     62.75k   \n",
            "15_transformer.h.1.attn.Dropout_resid_dropout      [32, 256, 250]          -   \n",
            "16_transformer.h.1.LayerNorm_ln_2                  [32, 256, 250]      500.0   \n",
            "17_transformer.h.1.mlp.Linear_c_fc                [32, 256, 1000]     251.0k   \n",
            "18_transformer.h.1.mlp.GELU_gelu                  [32, 256, 1000]          -   \n",
            "19_transformer.h.1.mlp.Linear_c_proj               [32, 256, 250]    250.25k   \n",
            "20_transformer.h.1.mlp.Dropout_dropout             [32, 256, 250]          -   \n",
            "21_transformer.h.2.LayerNorm_ln_1                  [32, 256, 250]      500.0   \n",
            "22_transformer.h.2.attn.Linear_c_attn              [32, 256, 750]    188.25k   \n",
            "23_transformer.h.2.attn.Linear_c_proj              [32, 256, 250]     62.75k   \n",
            "24_transformer.h.2.attn.Dropout_resid_dropout      [32, 256, 250]          -   \n",
            "25_transformer.h.2.LayerNorm_ln_2                  [32, 256, 250]      500.0   \n",
            "26_transformer.h.2.mlp.Linear_c_fc                [32, 256, 1000]     251.0k   \n",
            "27_transformer.h.2.mlp.GELU_gelu                  [32, 256, 1000]          -   \n",
            "28_transformer.h.2.mlp.Linear_c_proj               [32, 256, 250]    250.25k   \n",
            "29_transformer.h.2.mlp.Dropout_dropout             [32, 256, 250]          -   \n",
            "30_transformer.h.3.LayerNorm_ln_1                  [32, 256, 250]      500.0   \n",
            "31_transformer.h.3.attn.Linear_c_attn              [32, 256, 750]    188.25k   \n",
            "32_transformer.h.3.attn.Linear_c_proj              [32, 256, 250]     62.75k   \n",
            "33_transformer.h.3.attn.Dropout_resid_dropout      [32, 256, 250]          -   \n",
            "34_transformer.h.3.LayerNorm_ln_2                  [32, 256, 250]      500.0   \n",
            "35_transformer.h.3.mlp.Linear_c_fc                [32, 256, 1000]     251.0k   \n",
            "36_transformer.h.3.mlp.GELU_gelu                  [32, 256, 1000]          -   \n",
            "37_transformer.h.3.mlp.Linear_c_proj               [32, 256, 250]    250.25k   \n",
            "38_transformer.h.3.mlp.Dropout_dropout             [32, 256, 250]          -   \n",
            "39_transformer.h.4.LayerNorm_ln_1                  [32, 256, 250]      500.0   \n",
            "40_transformer.h.4.attn.Linear_c_attn              [32, 256, 750]    188.25k   \n",
            "41_transformer.h.4.attn.Linear_c_proj              [32, 256, 250]     62.75k   \n",
            "42_transformer.h.4.attn.Dropout_resid_dropout      [32, 256, 250]          -   \n",
            "43_transformer.h.4.LayerNorm_ln_2                  [32, 256, 250]      500.0   \n",
            "44_transformer.h.4.mlp.Linear_c_fc                [32, 256, 1000]     251.0k   \n",
            "45_transformer.h.4.mlp.GELU_gelu                  [32, 256, 1000]          -   \n",
            "46_transformer.h.4.mlp.Linear_c_proj               [32, 256, 250]    250.25k   \n",
            "47_transformer.h.4.mlp.Dropout_dropout             [32, 256, 250]          -   \n",
            "48_transformer.h.5.LayerNorm_ln_1                  [32, 256, 250]      500.0   \n",
            "49_transformer.h.5.attn.Linear_c_attn              [32, 256, 750]    188.25k   \n",
            "50_transformer.h.5.attn.Linear_c_proj              [32, 256, 250]     62.75k   \n",
            "51_transformer.h.5.attn.Dropout_resid_dropout      [32, 256, 250]          -   \n",
            "52_transformer.h.5.LayerNorm_ln_2                  [32, 256, 250]      500.0   \n",
            "53_transformer.h.5.mlp.Linear_c_fc                [32, 256, 1000]     251.0k   \n",
            "54_transformer.h.5.mlp.GELU_gelu                  [32, 256, 1000]          -   \n",
            "55_transformer.h.5.mlp.Linear_c_proj               [32, 256, 250]    250.25k   \n",
            "56_transformer.h.5.mlp.Dropout_dropout             [32, 256, 250]          -   \n",
            "57_transformer.h.6.LayerNorm_ln_1                  [32, 256, 250]      500.0   \n",
            "58_transformer.h.6.attn.Linear_c_attn              [32, 256, 750]    188.25k   \n",
            "59_transformer.h.6.attn.Linear_c_proj              [32, 256, 250]     62.75k   \n",
            "60_transformer.h.6.attn.Dropout_resid_dropout      [32, 256, 250]          -   \n",
            "61_transformer.h.6.LayerNorm_ln_2                  [32, 256, 250]      500.0   \n",
            "62_transformer.h.6.mlp.Linear_c_fc                [32, 256, 1000]     251.0k   \n",
            "63_transformer.h.6.mlp.GELU_gelu                  [32, 256, 1000]          -   \n",
            "64_transformer.h.6.mlp.Linear_c_proj               [32, 256, 250]    250.25k   \n",
            "65_transformer.h.6.mlp.Dropout_dropout             [32, 256, 250]          -   \n",
            "66_transformer.h.7.LayerNorm_ln_1                  [32, 256, 250]      500.0   \n",
            "67_transformer.h.7.attn.Linear_c_attn              [32, 256, 750]    188.25k   \n",
            "68_transformer.h.7.attn.Linear_c_proj              [32, 256, 250]     62.75k   \n",
            "69_transformer.h.7.attn.Dropout_resid_dropout      [32, 256, 250]          -   \n",
            "70_transformer.h.7.LayerNorm_ln_2                  [32, 256, 250]      500.0   \n",
            "71_transformer.h.7.mlp.Linear_c_fc                [32, 256, 1000]     251.0k   \n",
            "72_transformer.h.7.mlp.GELU_gelu                  [32, 256, 1000]          -   \n",
            "73_transformer.h.7.mlp.Linear_c_proj               [32, 256, 250]    250.25k   \n",
            "74_transformer.h.7.mlp.Dropout_dropout             [32, 256, 250]          -   \n",
            "75_transformer.h.8.LayerNorm_ln_1                  [32, 256, 250]      500.0   \n",
            "76_transformer.h.8.attn.Linear_c_attn              [32, 256, 750]    188.25k   \n",
            "77_transformer.h.8.attn.Linear_c_proj              [32, 256, 250]     62.75k   \n",
            "78_transformer.h.8.attn.Dropout_resid_dropout      [32, 256, 250]          -   \n",
            "79_transformer.h.8.LayerNorm_ln_2                  [32, 256, 250]      500.0   \n",
            "80_transformer.h.8.mlp.Linear_c_fc                [32, 256, 1000]     251.0k   \n",
            "81_transformer.h.8.mlp.GELU_gelu                  [32, 256, 1000]          -   \n",
            "82_transformer.h.8.mlp.Linear_c_proj               [32, 256, 250]    250.25k   \n",
            "83_transformer.h.8.mlp.Dropout_dropout             [32, 256, 250]          -   \n",
            "84_transformer.h.9.LayerNorm_ln_1                  [32, 256, 250]      500.0   \n",
            "85_transformer.h.9.attn.Linear_c_attn              [32, 256, 750]    188.25k   \n",
            "86_transformer.h.9.attn.Linear_c_proj              [32, 256, 250]     62.75k   \n",
            "87_transformer.h.9.attn.Dropout_resid_dropout      [32, 256, 250]          -   \n",
            "88_transformer.h.9.LayerNorm_ln_2                  [32, 256, 250]      500.0   \n",
            "89_transformer.h.9.mlp.Linear_c_fc                [32, 256, 1000]     251.0k   \n",
            "90_transformer.h.9.mlp.GELU_gelu                  [32, 256, 1000]          -   \n",
            "91_transformer.h.9.mlp.Linear_c_proj               [32, 256, 250]    250.25k   \n",
            "92_transformer.h.9.mlp.Dropout_dropout             [32, 256, 250]          -   \n",
            "93_transformer.h.10.LayerNorm_ln_1                 [32, 256, 250]      500.0   \n",
            "94_transformer.h.10.attn.Linear_c_attn             [32, 256, 750]    188.25k   \n",
            "95_transformer.h.10.attn.Linear_c_proj             [32, 256, 250]     62.75k   \n",
            "96_transformer.h.10.attn.Dropout_resid_dropout     [32, 256, 250]          -   \n",
            "97_transformer.h.10.LayerNorm_ln_2                 [32, 256, 250]      500.0   \n",
            "98_transformer.h.10.mlp.Linear_c_fc               [32, 256, 1000]     251.0k   \n",
            "99_transformer.h.10.mlp.GELU_gelu                 [32, 256, 1000]          -   \n",
            "100_transformer.h.10.mlp.Linear_c_proj             [32, 256, 250]    250.25k   \n",
            "101_transformer.h.10.mlp.Dropout_dropout           [32, 256, 250]          -   \n",
            "102_transformer.h.11.LayerNorm_ln_1                [32, 256, 250]      500.0   \n",
            "103_transformer.h.11.attn.Linear_c_attn            [32, 256, 750]    188.25k   \n",
            "104_transformer.h.11.attn.Linear_c_proj            [32, 256, 250]     62.75k   \n",
            "105_transformer.h.11.attn.Dropout_resid_dropout    [32, 256, 250]          -   \n",
            "106_transformer.h.11.LayerNorm_ln_2                [32, 256, 250]      500.0   \n",
            "107_transformer.h.11.mlp.Linear_c_fc              [32, 256, 1000]     251.0k   \n",
            "108_transformer.h.11.mlp.GELU_gelu                [32, 256, 1000]          -   \n",
            "109_transformer.h.11.mlp.Linear_c_proj             [32, 256, 250]    250.25k   \n",
            "110_transformer.h.11.mlp.Dropout_dropout           [32, 256, 250]          -   \n",
            "111_transformer.LayerNorm_ln_f                     [32, 256, 250]      500.0   \n",
            "112_lm_head                                      [32, 256, 50257]  12.56425M   \n",
            "\n",
            "                                                 Mult-Adds  \n",
            "Layer                                                       \n",
            "0_transformer.Embedding_wte                      12.56425M  \n",
            "1_transformer.Embedding_wpe                          64.0k  \n",
            "2_transformer.Dropout_drop                               -  \n",
            "3_transformer.h.0.LayerNorm_ln_1                     250.0  \n",
            "4_transformer.h.0.attn.Linear_c_attn                187.5k  \n",
            "5_transformer.h.0.attn.Linear_c_proj                 62.5k  \n",
            "6_transformer.h.0.attn.Dropout_resid_dropout             -  \n",
            "7_transformer.h.0.LayerNorm_ln_2                     250.0  \n",
            "8_transformer.h.0.mlp.Linear_c_fc                   250.0k  \n",
            "9_transformer.h.0.mlp.GELU_gelu                          -  \n",
            "10_transformer.h.0.mlp.Linear_c_proj                250.0k  \n",
            "11_transformer.h.0.mlp.Dropout_dropout                   -  \n",
            "12_transformer.h.1.LayerNorm_ln_1                    250.0  \n",
            "13_transformer.h.1.attn.Linear_c_attn               187.5k  \n",
            "14_transformer.h.1.attn.Linear_c_proj                62.5k  \n",
            "15_transformer.h.1.attn.Dropout_resid_dropout            -  \n",
            "16_transformer.h.1.LayerNorm_ln_2                    250.0  \n",
            "17_transformer.h.1.mlp.Linear_c_fc                  250.0k  \n",
            "18_transformer.h.1.mlp.GELU_gelu                         -  \n",
            "19_transformer.h.1.mlp.Linear_c_proj                250.0k  \n",
            "20_transformer.h.1.mlp.Dropout_dropout                   -  \n",
            "21_transformer.h.2.LayerNorm_ln_1                    250.0  \n",
            "22_transformer.h.2.attn.Linear_c_attn               187.5k  \n",
            "23_transformer.h.2.attn.Linear_c_proj                62.5k  \n",
            "24_transformer.h.2.attn.Dropout_resid_dropout            -  \n",
            "25_transformer.h.2.LayerNorm_ln_2                    250.0  \n",
            "26_transformer.h.2.mlp.Linear_c_fc                  250.0k  \n",
            "27_transformer.h.2.mlp.GELU_gelu                         -  \n",
            "28_transformer.h.2.mlp.Linear_c_proj                250.0k  \n",
            "29_transformer.h.2.mlp.Dropout_dropout                   -  \n",
            "30_transformer.h.3.LayerNorm_ln_1                    250.0  \n",
            "31_transformer.h.3.attn.Linear_c_attn               187.5k  \n",
            "32_transformer.h.3.attn.Linear_c_proj                62.5k  \n",
            "33_transformer.h.3.attn.Dropout_resid_dropout            -  \n",
            "34_transformer.h.3.LayerNorm_ln_2                    250.0  \n",
            "35_transformer.h.3.mlp.Linear_c_fc                  250.0k  \n",
            "36_transformer.h.3.mlp.GELU_gelu                         -  \n",
            "37_transformer.h.3.mlp.Linear_c_proj                250.0k  \n",
            "38_transformer.h.3.mlp.Dropout_dropout                   -  \n",
            "39_transformer.h.4.LayerNorm_ln_1                    250.0  \n",
            "40_transformer.h.4.attn.Linear_c_attn               187.5k  \n",
            "41_transformer.h.4.attn.Linear_c_proj                62.5k  \n",
            "42_transformer.h.4.attn.Dropout_resid_dropout            -  \n",
            "43_transformer.h.4.LayerNorm_ln_2                    250.0  \n",
            "44_transformer.h.4.mlp.Linear_c_fc                  250.0k  \n",
            "45_transformer.h.4.mlp.GELU_gelu                         -  \n",
            "46_transformer.h.4.mlp.Linear_c_proj                250.0k  \n",
            "47_transformer.h.4.mlp.Dropout_dropout                   -  \n",
            "48_transformer.h.5.LayerNorm_ln_1                    250.0  \n",
            "49_transformer.h.5.attn.Linear_c_attn               187.5k  \n",
            "50_transformer.h.5.attn.Linear_c_proj                62.5k  \n",
            "51_transformer.h.5.attn.Dropout_resid_dropout            -  \n",
            "52_transformer.h.5.LayerNorm_ln_2                    250.0  \n",
            "53_transformer.h.5.mlp.Linear_c_fc                  250.0k  \n",
            "54_transformer.h.5.mlp.GELU_gelu                         -  \n",
            "55_transformer.h.5.mlp.Linear_c_proj                250.0k  \n",
            "56_transformer.h.5.mlp.Dropout_dropout                   -  \n",
            "57_transformer.h.6.LayerNorm_ln_1                    250.0  \n",
            "58_transformer.h.6.attn.Linear_c_attn               187.5k  \n",
            "59_transformer.h.6.attn.Linear_c_proj                62.5k  \n",
            "60_transformer.h.6.attn.Dropout_resid_dropout            -  \n",
            "61_transformer.h.6.LayerNorm_ln_2                    250.0  \n",
            "62_transformer.h.6.mlp.Linear_c_fc                  250.0k  \n",
            "63_transformer.h.6.mlp.GELU_gelu                         -  \n",
            "64_transformer.h.6.mlp.Linear_c_proj                250.0k  \n",
            "65_transformer.h.6.mlp.Dropout_dropout                   -  \n",
            "66_transformer.h.7.LayerNorm_ln_1                    250.0  \n",
            "67_transformer.h.7.attn.Linear_c_attn               187.5k  \n",
            "68_transformer.h.7.attn.Linear_c_proj                62.5k  \n",
            "69_transformer.h.7.attn.Dropout_resid_dropout            -  \n",
            "70_transformer.h.7.LayerNorm_ln_2                    250.0  \n",
            "71_transformer.h.7.mlp.Linear_c_fc                  250.0k  \n",
            "72_transformer.h.7.mlp.GELU_gelu                         -  \n",
            "73_transformer.h.7.mlp.Linear_c_proj                250.0k  \n",
            "74_transformer.h.7.mlp.Dropout_dropout                   -  \n",
            "75_transformer.h.8.LayerNorm_ln_1                    250.0  \n",
            "76_transformer.h.8.attn.Linear_c_attn               187.5k  \n",
            "77_transformer.h.8.attn.Linear_c_proj                62.5k  \n",
            "78_transformer.h.8.attn.Dropout_resid_dropout            -  \n",
            "79_transformer.h.8.LayerNorm_ln_2                    250.0  \n",
            "80_transformer.h.8.mlp.Linear_c_fc                  250.0k  \n",
            "81_transformer.h.8.mlp.GELU_gelu                         -  \n",
            "82_transformer.h.8.mlp.Linear_c_proj                250.0k  \n",
            "83_transformer.h.8.mlp.Dropout_dropout                   -  \n",
            "84_transformer.h.9.LayerNorm_ln_1                    250.0  \n",
            "85_transformer.h.9.attn.Linear_c_attn               187.5k  \n",
            "86_transformer.h.9.attn.Linear_c_proj                62.5k  \n",
            "87_transformer.h.9.attn.Dropout_resid_dropout            -  \n",
            "88_transformer.h.9.LayerNorm_ln_2                    250.0  \n",
            "89_transformer.h.9.mlp.Linear_c_fc                  250.0k  \n",
            "90_transformer.h.9.mlp.GELU_gelu                         -  \n",
            "91_transformer.h.9.mlp.Linear_c_proj                250.0k  \n",
            "92_transformer.h.9.mlp.Dropout_dropout                   -  \n",
            "93_transformer.h.10.LayerNorm_ln_1                   250.0  \n",
            "94_transformer.h.10.attn.Linear_c_attn              187.5k  \n",
            "95_transformer.h.10.attn.Linear_c_proj               62.5k  \n",
            "96_transformer.h.10.attn.Dropout_resid_dropout           -  \n",
            "97_transformer.h.10.LayerNorm_ln_2                   250.0  \n",
            "98_transformer.h.10.mlp.Linear_c_fc                 250.0k  \n",
            "99_transformer.h.10.mlp.GELU_gelu                        -  \n",
            "100_transformer.h.10.mlp.Linear_c_proj              250.0k  \n",
            "101_transformer.h.10.mlp.Dropout_dropout                 -  \n",
            "102_transformer.h.11.LayerNorm_ln_1                  250.0  \n",
            "103_transformer.h.11.attn.Linear_c_attn             187.5k  \n",
            "104_transformer.h.11.attn.Linear_c_proj              62.5k  \n",
            "105_transformer.h.11.attn.Dropout_resid_dropout          -  \n",
            "106_transformer.h.11.LayerNorm_ln_2                  250.0  \n",
            "107_transformer.h.11.mlp.Linear_c_fc                250.0k  \n",
            "108_transformer.h.11.mlp.GELU_gelu                       -  \n",
            "109_transformer.h.11.mlp.Linear_c_proj              250.0k  \n",
            "110_transformer.h.11.mlp.Dropout_dropout                 -  \n",
            "111_transformer.LayerNorm_ln_f                       250.0  \n",
            "112_lm_head                                      12.56425M  \n",
            "-------------------------------------------------------------------------------------------------------\n",
            "                         Totals\n",
            "Total params            34.232M\n",
            "Trainable params        34.232M\n",
            "Non-trainable params        0.0\n",
            "Mult-Adds             34.19875M\n",
            "=======================================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7d2ce572-1bb7-41b3-8798-c45571499f53\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_transformer.Embedding_wte</th>\n",
              "      <td>[250, 50257]</td>\n",
              "      <td>[32, 256, 250]</td>\n",
              "      <td>12564250.0</td>\n",
              "      <td>12564250.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_transformer.Embedding_wpe</th>\n",
              "      <td>[250, 256]</td>\n",
              "      <td>[256, 250]</td>\n",
              "      <td>64000.0</td>\n",
              "      <td>64000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_transformer.Dropout_drop</th>\n",
              "      <td>-</td>\n",
              "      <td>[32, 256, 250]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_transformer.h.0.LayerNorm_ln_1</th>\n",
              "      <td>[250]</td>\n",
              "      <td>[32, 256, 250]</td>\n",
              "      <td>500.0</td>\n",
              "      <td>250.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_transformer.h.0.attn.Linear_c_attn</th>\n",
              "      <td>[250, 750]</td>\n",
              "      <td>[32, 256, 750]</td>\n",
              "      <td>188250.0</td>\n",
              "      <td>187500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108_transformer.h.11.mlp.GELU_gelu</th>\n",
              "      <td>-</td>\n",
              "      <td>[32, 256, 1000]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109_transformer.h.11.mlp.Linear_c_proj</th>\n",
              "      <td>[1000, 250]</td>\n",
              "      <td>[32, 256, 250]</td>\n",
              "      <td>250250.0</td>\n",
              "      <td>250000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110_transformer.h.11.mlp.Dropout_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[32, 256, 250]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111_transformer.LayerNorm_ln_f</th>\n",
              "      <td>[250]</td>\n",
              "      <td>[32, 256, 250]</td>\n",
              "      <td>500.0</td>\n",
              "      <td>250.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112_lm_head</th>\n",
              "      <td>[250, 50257]</td>\n",
              "      <td>[32, 256, 50257]</td>\n",
              "      <td>12564250.0</td>\n",
              "      <td>12564250.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>113 rows  4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d2ce572-1bb7-41b3-8798-c45571499f53')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7d2ce572-1bb7-41b3-8798-c45571499f53 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7d2ce572-1bb7-41b3-8798-c45571499f53');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-df574c3a-6966-46ac-8d49-a9f15c50531d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-df574c3a-6966-46ac-8d49-a9f15c50531d')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-df574c3a-6966-46ac-8d49-a9f15c50531d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                          Kernel Shape      Output Shape  \\\n",
              "Layer                                                                      \n",
              "0_transformer.Embedding_wte               [250, 50257]    [32, 256, 250]   \n",
              "1_transformer.Embedding_wpe                 [250, 256]        [256, 250]   \n",
              "2_transformer.Dropout_drop                           -    [32, 256, 250]   \n",
              "3_transformer.h.0.LayerNorm_ln_1                 [250]    [32, 256, 250]   \n",
              "4_transformer.h.0.attn.Linear_c_attn        [250, 750]    [32, 256, 750]   \n",
              "...                                                ...               ...   \n",
              "108_transformer.h.11.mlp.GELU_gelu                   -   [32, 256, 1000]   \n",
              "109_transformer.h.11.mlp.Linear_c_proj     [1000, 250]    [32, 256, 250]   \n",
              "110_transformer.h.11.mlp.Dropout_dropout             -    [32, 256, 250]   \n",
              "111_transformer.LayerNorm_ln_f                   [250]    [32, 256, 250]   \n",
              "112_lm_head                               [250, 50257]  [32, 256, 50257]   \n",
              "\n",
              "                                              Params   Mult-Adds  \n",
              "Layer                                                             \n",
              "0_transformer.Embedding_wte               12564250.0  12564250.0  \n",
              "1_transformer.Embedding_wpe                  64000.0     64000.0  \n",
              "2_transformer.Dropout_drop                       NaN         NaN  \n",
              "3_transformer.h.0.LayerNorm_ln_1               500.0       250.0  \n",
              "4_transformer.h.0.attn.Linear_c_attn        188250.0    187500.0  \n",
              "...                                              ...         ...  \n",
              "108_transformer.h.11.mlp.GELU_gelu               NaN         NaN  \n",
              "109_transformer.h.11.mlp.Linear_c_proj      250250.0    250000.0  \n",
              "110_transformer.h.11.mlp.Dropout_dropout         NaN         NaN  \n",
              "111_transformer.LayerNorm_ln_f                 500.0       250.0  \n",
              "112_lm_head                               12564250.0  12564250.0  \n",
              "\n",
              "[113 rows x 4 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = GPT().to(device)\n",
        "summary(model, x.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e40mMxslJ04I"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss() # Defining Loss function.\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr= config['init_lr']) # Defining Optimizer\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLRonPlateau(optimizer, patience=2, factor=0.75)\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEcwsdFJKQAM"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "\n",
        "    model.train()\n",
        "    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    counter = 0\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(dataloader):\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "        ### Initialize Gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "          ### Move Data to Device (Ideally GPU)\n",
        "          inputs      = inputs.to(device)\n",
        "          targets    = targets.to(device)\n",
        "\n",
        "          ### Forward Propagation\n",
        "          logits  = model(inputs)\n",
        "\n",
        "          ### Loss Calculation\n",
        "          logits = logits.view(-1, logits.size(-1))\n",
        "          targets = targets.view(-1)\n",
        "          loss = F.cross_entropy(logits, targets, ignore_index=-1)\n",
        "\n",
        "        ### Backward Propagation\n",
        "        scaler.scale(loss).backward()\n",
        "        # loss.backward()\n",
        "\n",
        "        ### Gradient Descent\n",
        "        scaler.step(optimizer)\n",
        "\n",
        "        scaler.update()\n",
        "\n",
        "        tloss   += loss.item()\n",
        "        tacc    += torch.sum(torch.argmax(logits, dim= 1) == targets).item()/logits.shape[0]\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n",
        "                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        if (counter % 1000 == 0):\n",
        "          wandb.log({'train_acc': (tacc/counter)*100, 'train_loss': (tloss/counter), 'lr': curr_lr})\n",
        "\n",
        "        ### Release memory\n",
        "        del inputs, targets, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    tloss   /= len(train_loader)\n",
        "    tacc    /= len(train_loader)\n",
        "\n",
        "    return tloss, tacc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUmX0T-hNIyh"
      },
      "outputs": [],
      "source": [
        "def eval(model, dataloader):\n",
        "\n",
        "    model.eval() # set model in evaluation mode\n",
        "    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(dataloader):\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "        ### Move data to device (ideally GPU)\n",
        "        inputs      = inputs.to(device)\n",
        "        targets    = targets.to(device)\n",
        "\n",
        "        # makes sure that there are no gradients computed as we are not training the model now\n",
        "        with torch.inference_mode():\n",
        "            ### Forward Propagation\n",
        "            logits  = model(inputs)\n",
        "            ### Loss Calculation\n",
        "            logits = logits.view(-1, logits.size(-1))\n",
        "            targets = targets.view(-1)\n",
        "            loss = F.cross_entropy(logits, targets, ignore_index=-1)\n",
        "\n",
        "        vloss   += loss.item()\n",
        "        vacc    += torch.sum(torch.argmax(logits, dim= 1) == targets).item()/logits.shape[0]\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))),\n",
        "                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        if (counter % 100 == 0):\n",
        "          wandb.log({'val_acc': (vacc/counter)*100, 'val_loss': (vloss/counter), 'lr': curr_lr})\n",
        "\n",
        "        ### Release memory\n",
        "        del inputs, targets, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    vloss   /= len(val_loader)\n",
        "    vacc    /= len(val_loader)\n",
        "\n",
        "    return vloss, vacc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgoLXmGTNiiW",
        "outputId": "5fb60cfd-a57d-4bb4-d18f-9e3737818acc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkkmittal\u001b[0m (\u001b[33midl-f23\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login(key=\"{OMITTED}\") # API key for the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "7JDo6TRsNpTZ",
        "outputId": "6796dc19-e7d8-45b1-8aca-6845175813ee"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231128_033038-z0jos8j3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idl-f23/hw5/runs/z0jos8j3' target=\"_blank\">low-batch-high-block</a></strong> to <a href='https://wandb.ai/idl-f23/hw5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/idl-f23/hw5' target=\"_blank\">https://wandb.ai/idl-f23/hw5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/idl-f23/hw5/runs/z0jos8j3' target=\"_blank\">https://wandb.ai/idl-f23/hw5/runs/z0jos8j3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create your wandb run\n",
        "run = wandb.init(\n",
        "    name    = \"low-batch-high-block\", ### Wandb last name initializer\n",
        "    reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    project = \"hw5\", ### Project should be created in WandB\n",
        "    config  = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCh0vVfVNYl9",
        "outputId": "1f969e14-0946-497c-e18e-754fde747dbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 40.0969%\tTrain Loss 3.6737\t Learning Rate 0.0000300\n",
            "\tVal Acc 42.8086%\tVal Loss 3.5446\n",
            "\n",
            "Epoch 6/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train:  72%|  | 14079/19511 [1:27:25<33:41,  2.69it/s, acc=40.3539%, loss=3.6506]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buffered data was truncated after reaching the output size limit."
          ]
        }
      ],
      "source": [
        "# Iterate over number of epochs to train and evaluate your model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "for epoch in range(config['epochs']+20):\n",
        "\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch, config['epochs']+20))\n",
        "\n",
        "    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n",
        "    train_loss, train_acc   = train(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_acc       = eval(model, val_loader)\n",
        "\n",
        "    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n",
        "    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n",
        "\n",
        "    wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n",
        "               'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})\n",
        "\n",
        "    scheduler.step(val_loss)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
