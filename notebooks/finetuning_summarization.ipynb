{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i29BVVlUdbpQ"
      },
      "source": [
        "## Import Libraries and Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENFSqOnlQsB6"
      },
      "outputs": [],
      "source": [
        "!pip install wandb --quiet -q\n",
        "!pip install torchsummaryX -q\n",
        "!pip install datasets -q\n",
        "!pip install zstandard -q\n",
        "!pip install tiktoken -q\n",
        "!pip install rouge -q\n",
        "!pip install torch nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhOGy0oVQR2z",
        "outputId": "4dde8631-7f6a-4bf0-a54b-90123444dc15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio.transforms as tat\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from rouge import Rouge\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import zstandard\n",
        "import datasets\n",
        "import tiktoken\n",
        "import random\n",
        "import wandb\n",
        "import math\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa2w5xXiU6OV"
      },
      "outputs": [],
      "source": [
        "### If you are using colab, you can import google drive to save model checkpoints in a folder.\n",
        "### This is used when connecting to GCE VMs, but the user still wants to connect to Google Drive\n",
        "import os.path as path\n",
        "if not path.exists(\"/content/drive\"):\n",
        "  !sudo add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "  !sudo apt-get update -qq 2>&1 > /dev/null\n",
        "  !sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null\n",
        "  !google-drive-ocamlfuse\n",
        "\n",
        "  !sudo apt-get install -qq w3m # to act as web browser\n",
        "  !xdg-settings set default-web-browser w3m.desktop # to set default browser\n",
        "  %cd /content\n",
        "  !mkdir drive\n",
        "  %cd drive\n",
        "  !mkdir MyDrive\n",
        "  %cd ..\n",
        "  %cd ..\n",
        "  !google-drive-ocamlfuse /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "qmUzYbotBJOm"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'epochs'        : 5,\n",
        "    'batch_size'    : 64,\n",
        "    'init_lr'       : 3e-5,\n",
        "    'block_size'    : 256,\n",
        "    'dropout'       : 0.1,\n",
        "    'vocab_size'    : 50257,\n",
        "    'bias'          : True,\n",
        "    'n_layer'       : 12,\n",
        "    'n_head'        : 10,\n",
        "    'n_embd'        : 250,\n",
        "    'end_token'     : 50256,\n",
        "    'summary_length' : 30\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxZSF7rroGzp"
      },
      "source": [
        "## Blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p50Gn9cdoGzz"
      },
      "outputs": [],
      "source": [
        "# Layer normalization for regularizing the model\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, ndim, bias):\n",
        "    super().__init__()\n",
        "    self.weight, self.bias = nn.Parameter(torch.ones(ndim)), nn.Parameter(torch.zeros(ndim))\n",
        "\n",
        "  def forward(self, input):\n",
        "    return nn.functional.layer_norm(input=input, \n",
        "                                    normalized_shape=self.weight.shape, \n",
        "                                    weight=self.weight, \n",
        "                                    bias=self.bias, \n",
        "                                    eps=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlVCrSodoGzz"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.attention_layer = nn.Linear(config['n_embd'], 3 * config['n_embd'])\n",
        "    self.projection_layer = nn.Linear(config['n_embd'], config['n_embd'])\n",
        "    self.dropout = nn.Dropout(config['dropout'])\n",
        "\n",
        "  def attention_calculation (self, x):\n",
        "    query, key, value = self.attention_layer(x).split(config['n_embd'], dim=2)\n",
        "    key = key.view(x.size(0), x.size(1), config['n_head'], x.size(2) // config['n_head'])\n",
        "    query = query.view(x.size(0), x.size(1), config['n_head'], x.size(2) // config['n_head'])\n",
        "    value = value.view(x.size(0), x.size(1), config['n_head'], x.size(2) // config['n_head'])\n",
        "\n",
        "    key, query, value = key.transpose(1, 2), query.transpose(1, 2), value.transpose(1, 2)\n",
        "\n",
        "    y = nn.functional.scaled_dot_product_attention(query, key, value, dropout_p=config['dropout'])\n",
        "\n",
        "  def forward(self, x):\n",
        "    y = self.attention_calculation(x)\n",
        "    out = y.transpose(1, 2).view(x.size(0), x.size(1), x.size(2))\n",
        "    out = self.projection_layer(out)\n",
        "    out = self.dropout(out)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPbuhh7XoGzz"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config['n_embd'], bias=config['bias'])\n",
        "        self.attn = AttentionLayer()\n",
        "        self.ln_2 = LayerNorm(config['n_embd'], bias=config['bias'])\n",
        "        self.mlp = nn.Sequential (\n",
        "            nn.Linear(config['n_embd'], 4 * config['n_embd']),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config['n_embd'], config['n_embd']),\n",
        "            nn.Dropout(config['dropout'])\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EDtZ3PcoGzz"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Equl2QngoGzz"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embed = nn.Embedding(config['vocab_size'], config['n_embd'])\n",
        "    self.pos_embed = nn.Embedding(config['block_size'], config['n_embd'])\n",
        "    self.dropout = nn.Dropout(config['dropout'])\n",
        "\n",
        "    modules = [Block() for b in range(config['n_layer'])]\n",
        "    self.blocks = nn.Sequential(*modules)\n",
        "    self.layernorm = LayerNorm(config['n_embd'], bias=config['bias'])\n",
        "\n",
        "    self.lin1 = nn.Linear(config['n_embd'], config['vocab_size'])\n",
        "    self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "  def forward(self, idx):\n",
        "      position = torch.arange(0, idx.size(1))\n",
        "\n",
        "      tok_emb = self.token_embed(idx)\n",
        "      pos_emb = self.pos_embed(position)\n",
        "      x = self.dropout(tok_emb + pos_emb)\n",
        "\n",
        "      for block in self.blocks: x = block(x)\n",
        "      x = self.layernorm(x)\n",
        "\n",
        "      return self.lin1(x)\n",
        "\n",
        "  def generate(self, idx, max_new_tokens, temperature=1.0):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx if idx.size(1) <= self.config['block_size'] else idx[:, -self.config['block_size']:]\n",
        "        logits, _ = self(idx_cond)\n",
        "        probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlTHyR5ZB3Ym"
      },
      "source": [
        "## Load CNN/Daily Mail Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TI3NFBuGQvpQ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"cnn_dailymail\", '3.0.0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTmDPzZpvcUQ",
        "outputId": "98baa6f4-a6b8-4d63-d454-f04876a41740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['train', 'validation', 'test'])\n"
          ]
        }
      ],
      "source": [
        "print(dataset.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkuC9VJ7YNGN"
      },
      "outputs": [],
      "source": [
        "END_OF_TEXT = 50256\n",
        "START_OF_TEXT = 50255\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "summarization_prompt = enc.encode_ordinary(\"Summarize this article:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQyh2CMJPLFD"
      },
      "outputs": [],
      "source": [
        "def split_into_chunks(encoded_article, article_index, chunk_size=250):\n",
        "    end_range = len(encoded_article) - len(encoded_article) % chunk_size\n",
        "    return [(summarization_prompt + encoded_article[i:i + chunk_size], article_index) for i in range(0, end_range, chunk_size)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BT6Lc3SsP4qg"
      },
      "outputs": [],
      "source": [
        "def pad_with_eos(text, length=config['summary_length']):\n",
        "  if length > len(text):\n",
        "    pad_len = length - len(text)\n",
        "    text += [END_OF_TEXT] * pad_len\n",
        "    return text\n",
        "  else:\n",
        "    return text[:length]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5qmyL2DJAT3"
      },
      "outputs": [],
      "source": [
        "def flatten_extend(matrix):\n",
        "    flat_list = []\n",
        "    for row in matrix:\n",
        "      flat_list.extend(row)\n",
        "    return flat_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOjoSZf8Y3kr"
      },
      "source": [
        "# Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5cDvJ8E_hfQ"
      },
      "outputs": [],
      "source": [
        "# Dataset class to load train and validation data\n",
        "\n",
        "class CNNDailyMailDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, prefix, encoder):\n",
        "\n",
        "        data = dataset[prefix]\n",
        "\n",
        "        self.enc = encoder\n",
        "\n",
        "        self.inputs = [split_into_chunks(self.enc.encode_ordinary(data[i][\"article\"]), i)\n",
        "            if (data[i][\"article\"] != None) else [] for i in range(len(data))]\n",
        "\n",
        "        self.inputs = flatten_extend(self.inputs)\n",
        "\n",
        "        self.targets = np.array([pad_with_eos(enc.encode_ordinary(data[i][\"highlights\"])) for i in range(len(data))])\n",
        "\n",
        "        self.length = len(self.inputs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        article_chunk = self.inputs[ind][0]\n",
        "        article_index = self.inputs[ind][1]\n",
        "        return torch.tensor(article_chunk), torch.tensor(self.targets[article_index]), article_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnKRglS4-l4Z"
      },
      "outputs": [],
      "source": [
        "class CNNDailyMailTestDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, prefix, encoder):\n",
        "\n",
        "        data = dataset[prefix]\n",
        "\n",
        "        self.enc = encoder\n",
        "\n",
        "        self.inputs = [split_into_chunks(self.enc.encode_ordinary(data[i][\"article\"]), i)\n",
        "            if (data[i][\"article\"] != None) else [] for i in range(len(data))]\n",
        "\n",
        "        self.targets = np.array([data[i]['highlights'] for i in range(len(data))])\n",
        "\n",
        "        self.length = len(self.inputs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        return self.inputs[ind], self.targets[ind]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIrqy1cDAzBX"
      },
      "outputs": [],
      "source": [
        "train_data = CNNDailyMailDataset(prefix=\"train\", encoder=enc)\n",
        "val_data = CNNDailyMailDataset(prefix=\"validation\",encoder=enc)\n",
        "test_data = CNNDailyMailTestDataset(prefix=\"test\",encoder=enc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h8A-oEVA9Rw"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "     dataset     = train_data,\n",
        "     num_workers = 1,\n",
        "     batch_size  = config['batch_size'],\n",
        "     pin_memory  = True,\n",
        "     drop_last   = True,\n",
        "     shuffle     = True\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "     dataset     = val_data,\n",
        "     num_workers = 1,\n",
        "     batch_size  = config['batch_size'],\n",
        "     pin_memory  = True,\n",
        "     drop_last   = True,\n",
        "     shuffle     = False\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_data,\n",
        "    num_workers = 1,\n",
        "    batch_size  = 1,\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4er1tTL8BNzo",
        "outputId": "c74e4de8-9a96-42f5-9dbe-41052cd016e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 256]) torch.Size([64, 30])\n",
            "tensor([153355, 124372, 191885, 248379, 175910, 116445,  22201, 104562, 142908,\n",
            "         59902, 175479, 231670, 272493, 126000, 217487, 263987, 177404, 132113,\n",
            "        173588, 224715,  95684, 275608, 105229,  23827, 192049, 215502,  30395,\n",
            "        172546, 212987, 269283, 264306,  57238,  31544,  30474, 173563, 176491,\n",
            "         60742, 148873, 135951, 225883,  33081, 243449,  62388, 230082, 179230,\n",
            "        274306, 261521,  90175, 199905,  62483, 146215, 169031, 270411,  34565,\n",
            "        264669,  92047, 183236,  41192, 273252, 161119, 108714, 132067, 245069,\n",
            "        215919])\n",
            "tensor([[13065,  3876,  1096,  ..., 12526,  1683,  2826],\n",
            "        [13065,  3876,  1096,  ...,  9074, 26618,  2087],\n",
            "        [13065,  3876,  1096,  ...,  2863,    11,   772],\n",
            "        ...,\n",
            "        [13065,  3876,  1096,  ...,  2263,  5986,   286],\n",
            "        [13065,  3876,  1096,  ...,  1521,   326, 30597],\n",
            "        [13065,  3876,  1096,  ...,    11,   705,  7109]]) tensor([[31407,   318,   517,  ...,   284,   711,  8685],\n",
            "        [18308, 26618,   373,  ...,  2237,    12,  8589],\n",
            "        [19962, 15868,   979,  ..., 10033,  4302,  6075],\n",
            "        ...,\n",
            "        [ 5167,   621,  7337,  ...,  1542,  2693,   287],\n",
            "        [25631, 33508,  8565,  ...,  8837,   468,   300],\n",
            "        [   50, 16809,    75,  ...,   286,  1719,  1813]])\n"
          ]
        }
      ],
      "source": [
        "# Testing code to check if your data loaders are working\n",
        "for i, data in enumerate(train_loader):\n",
        "     x, y, article_idx = data\n",
        "     print(x.shape, y.shape)\n",
        "     print(article_idx)\n",
        "     print(x, y)\n",
        "     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If2zw_RvzwHC",
        "outputId": "cad886ac-61a1-4b60-8253-9a9475cd1c0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0])\n",
            "['Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\\nIsrael and the United States opposed the move, which could open the door to war crimes investigations against Israelis .']\n"
          ]
        }
      ],
      "source": [
        "for i, data in enumerate(test_loader):\n",
        "  x, y = data\n",
        "  print(x[1][1])\n",
        "  print(y)\n",
        "  break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7duoQz7ZBIa"
      },
      "source": [
        "# Load Pretrained Model from Checkpoint / Optimizer / Criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rlmmrv3CWy09",
        "outputId": "6fa96462-8cbd-4e4a-994f-13e03f91a6d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# prompt: Instantiate and load a model from a checkpoint file\n",
        "\n",
        "model = GPT().to(device)\n",
        "checkpoint_path = './pretrained_model_checkpoint.pth'\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "model.load_state_dict(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "_tFDfPECtCO6"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=END_OF_TEXT)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr= config['init_lr']) #Defining Optimizer\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.75)\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1TCHUMdkdZb",
        "outputId": "da492ee4-9577-43f0-9e23-e23761b37d52"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKvcyzUgAven"
      },
      "outputs": [],
      "source": [
        "def generate_text_logits(model, seq, max_new_tokens=config['summary_length']):\n",
        "  text_logits = None\n",
        "\n",
        "  for i in range(max_new_tokens):\n",
        "      # if the sequence context is growing too long we must crop it at block_size\n",
        "      seq_cond = seq if seq.size(1) <= config['block_size'] else seq[:, -config['block_size']:]\n",
        "      # forward the model to get the logits for the index in the sequence\n",
        "      logits = model(seq_cond)\n",
        "\n",
        "      if i == max_new_tokens-1:\n",
        "        text_logits = logits[:, -max_new_tokens:, :] # batch size, max_new_tokens, vocab_size\n",
        "\n",
        "      # pluck the logits at the final step and scale by desired temperature\n",
        "      logits = logits[:, -1, :]\n",
        "      # apply softmax to convert logits to (normalized) probabilities\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      # append sampled index to the running sequence and continue\n",
        "      seq = torch.cat((seq, idx_next), dim=1)\n",
        "\n",
        "  return text_logits, seq[:, -max_new_tokens:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "gYJPadjWlHoW"
      },
      "outputs": [],
      "source": [
        "def calculate_bleu_score(reference, candidate):\n",
        "    \"\"\"\n",
        "    Calculate the average BLEU score for a batch of translations.\n",
        "\n",
        "    Args:\n",
        "    - references_batch: A list of lists, where each inner list contains a single reference translation.\n",
        "    - candidates_batch: A list of candidate translations.\n",
        "\n",
        "    Returns:\n",
        "    - average_bleu_score: The average BLEU score for the entire batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the strings\n",
        "    reference_tokenized = nltk.word_tokenize(reference)\n",
        "    candidate_tokenized = nltk.word_tokenize(candidate)\n",
        "\n",
        "    # Calculate BLEU score for each translation in the batch\n",
        "    bleu_score = corpus_bleu([reference_tokenized], [candidate_tokenized])\n",
        "\n",
        "    return bleu_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doO9W80hZLq1"
      },
      "source": [
        "# Train, Eval, Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EynfjERYLMe"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "\n",
        "    model.train()\n",
        "    tloss = 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    counter = 0\n",
        "\n",
        "    for i, (inputs, targets, article_idx) in enumerate(dataloader):\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "        ### Initialize Gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "          ### Move Data to Device (Ideally GPU)\n",
        "          inputs      = inputs.to(device)\n",
        "          targets    = targets.to(device)\n",
        "\n",
        "          ### Forward Propagation\n",
        "          logits, _ = generate_text_logits(model, inputs)\n",
        "\n",
        "          B, T, C = logits.shape\n",
        "\n",
        "          logits = logits.reshape(B*T, -1)\n",
        "          targets = targets.reshape(-1)\n",
        "\n",
        "          loss =  criterion(logits, targets)\n",
        "\n",
        "\n",
        "        ### Backward Propagation\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        ### Gradient Descent\n",
        "        scaler.step(optimizer)\n",
        "\n",
        "        scaler.update()\n",
        "\n",
        "        tloss   += loss.item()\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        if counter % 50 == 0:\n",
        "          wandb.log({'train_loss': (tloss/counter), 'lr': curr_lr})\n",
        "\n",
        "\n",
        "        ### Release memory\n",
        "        del inputs, targets, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    tloss   /= len(dataloader)\n",
        "\n",
        "    return tloss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "twhD90IsYjQ2"
      },
      "outputs": [],
      "source": [
        "def eval(model, dataloader):\n",
        "\n",
        "    model.eval() # set model in evaluation mode\n",
        "    vloss = 0 # Monitoring loss, accuracy, and distance\n",
        "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    for i, (inputs, targets, article_idx) in enumerate(dataloader):\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "        ### Move data to device (ideally GPU)\n",
        "        inputs      = inputs.to(device)\n",
        "        targets    = targets.to(device)\n",
        "\n",
        "        # makes sure that there are no gradients computed as we are not training the model now\n",
        "        with torch.inference_mode():\n",
        "            logits, text = generate_text_logits(model, inputs)\n",
        "\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits = logits.reshape(B*T, -1)\n",
        "            targets = targets.reshape(-1)\n",
        "\n",
        "            loss =  criterion(logits, targets)\n",
        "\n",
        "        # strip\n",
        "        vloss   += loss.item()\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.07f}\".format(float(vloss / (i + 1))))\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "        if counter % 50 == 0:\n",
        "          wandb.log({'val_loss': (vloss/counter), 'dist/lr': curr_lr})\n",
        "\n",
        "        ### Release memory\n",
        "        del inputs, targets, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    vloss   /= len(dataloader)\n",
        "\n",
        "    return vloss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "fMivH6Uuqpcx"
      },
      "outputs": [],
      "source": [
        "def test(model, dataloader):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n",
        "\n",
        "  counter = 0\n",
        "\n",
        "  bleu_score = 0.0\n",
        "\n",
        "  for i, (inputs, targets) in enumerate(dataloader):\n",
        "\n",
        "    counter += 1\n",
        "\n",
        "    generated_summary = []\n",
        "\n",
        "    for chunk in inputs:\n",
        "      chunk = torch.tensor([chunk[0]]).to(device)\n",
        "      _ , text = generate_text_logits(model, chunk)\n",
        "      truncated_text = []\n",
        "      for token in text[0]:\n",
        "        if token == END_OF_TEXT:\n",
        "          break\n",
        "        truncated_text.append(token)\n",
        "      generated_summary.extend(truncated_text)\n",
        "\n",
        "    generated_summary = enc.decode(generated_summary)\n",
        "\n",
        "\n",
        "    bleu_score += calculate_bleu_score(targets[0], generated_summary)\n",
        "\n",
        "    print(bleu_score/(i+1))\n",
        "\n",
        "    batch_bar.set_postfix(score=\"{:.07f}\".format(float(bleu_score / (i + 1))))\n",
        "    batch_bar.update()\n",
        "\n",
        "  batch_bar.close()\n",
        "  bleu_score /= len(dataloader)\n",
        "\n",
        "  return bleu_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxwCY3QLZOwx"
      },
      "source": [
        "# WandB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgoLXmGTNiiW",
        "outputId": "b09c842d-7208-42c5-f8cb-9eb24d459323"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkkmittal\u001b[0m (\u001b[33midl-f23\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login(key=\"9312acc23a6389a925ba54b1bdf81ff99fe4d2e4\") # API key for the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JDo6TRsNpTZ",
        "outputId": "c60c8a3e-89c7-4abd-edd0-a5295996d139"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231211_215308-7t817bz5</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idl-f23/hw5-finetune/runs/7t817bz5' target=\"_blank\">summarization-finetuning</a></strong> to <a href='https://wandb.ai/idl-f23/hw5-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/idl-f23/hw5-finetune' target=\"_blank\">https://wandb.ai/idl-f23/hw5-finetune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/idl-f23/hw5-finetune/runs/7t817bz5' target=\"_blank\">https://wandb.ai/idl-f23/hw5-finetune/runs/7t817bz5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create your wandb run\n",
        "run = wandb.init(\n",
        "    name    = \"summarization-finetuning\", ### Wandb last name initializer\n",
        "    reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    project = \"hw5-finetune\", ### Project should be created in WandB\n",
        "    config  = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ickbLqAZVpK"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlStyodfkXdy"
      },
      "outputs": [],
      "source": [
        "# Iterate over number of epochs to train and evaluate your model\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch, config['epochs']))\n",
        "\n",
        "    curr_lr      = float(optimizer.param_groups[0]['lr'])\n",
        "    train_loss   = train(model, train_loader, optimizer, criterion)\n",
        "    val_loss     = eval(model, val_loader)\n",
        "\n",
        "    print(\"\\tTrain Loss {:.07f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
        "    print(\"\\tVal Loss {:.07f}\\t\".format(val_loss))\n",
        "\n",
        "    wandb.log({'train_loss': train_loss, 'valid_loss': val_loss, 'lr': curr_lr})\n",
        "\n",
        "    ### Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best\n",
        "    torch.save({'model_state_dict' : model.state_dict(), 'optimizer_state_dict' : optimizer.state_dict()}, '/content/Finetuning/Summarization/checkpoint_epoch_' + str(epoch) + '.pth')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      torch.save({'model_state_dict' : model.state_dict(), 'optimizer_state_dict' : optimizer.state_dict()}, '/content/Finetuning/Summarization/checkpoint_best.pth')\n",
        "\n",
        "    scheduler.step(val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9MYf0ALZabo"
      },
      "source": [
        "# Testing / Calculate Bleu Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sgb4lw6Ux3ig",
        "outputId": "3b4c1d7c-46da-46c0-daba-ce55a4f5cee0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.5612842184142e-32\n"
          ]
        }
      ],
      "source": [
        "bleu_score = test(model, test_loader)\n",
        "print(bleu_score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BxZSF7rroGzp",
        "4EDtZ3PcoGzz"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
