{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i29BVVlUdbpQ"
      },
      "source": [
        "## Import Libraries and Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENFSqOnlQsB6",
        "outputId": "7409b08e-b570-4dee-a6fe-8060b34b618b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb --quiet -q\n",
        "!pip install torchsummaryX -q\n",
        "!pip install datasets -q\n",
        "!pip install zstandard -q\n",
        "!pip install tiktoken -q\n",
        "!pip install rouge -q\n",
        "!pip install torch nltk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AVVJFqnL3jC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhOGy0oVQR2z",
        "outputId": "4dde8631-7f6a-4bf0-a54b-90123444dc15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio.transforms as tat\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from rouge import Rouge\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import zstandard\n",
        "import datasets\n",
        "import tiktoken\n",
        "import random\n",
        "import wandb\n",
        "import math\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa2w5xXiU6OV"
      },
      "outputs": [],
      "source": [
        "### If you are using colab, you can import google drive to save model checkpoints in a folder.\n",
        "### This is used when connecting to GCE VMs, but the user still wants to connect to Google Drive\n",
        "import os.path as path\n",
        "if not path.exists(\"/content/drive\"):\n",
        "  !sudo add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "  !sudo apt-get update -qq 2>&1 > /dev/null\n",
        "  !sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null\n",
        "  !google-drive-ocamlfuse\n",
        "\n",
        "  !sudo apt-get install -qq w3m # to act as web browser\n",
        "  !xdg-settings set default-web-browser w3m.desktop # to set default browser\n",
        "  %cd /content\n",
        "  !mkdir drive\n",
        "  %cd drive\n",
        "  !mkdir MyDrive\n",
        "  %cd ..\n",
        "  %cd ..\n",
        "  !google-drive-ocamlfuse /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmUzYbotBJOm"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'epochs'        : 5,\n",
        "    'batch_size'    : 64,\n",
        "    'init_lr'       : 3e-4,\n",
        "    'block_size'    : 256,\n",
        "    'dropout'       : 0.1,\n",
        "    'vocab_size'    : 50257,\n",
        "    'bias'          : True,\n",
        "    'n_layer'       : 12,\n",
        "    'n_head'        : 10,\n",
        "    'n_embd'        : 250,\n",
        "    'end_token'     : 50256,\n",
        "    'summary_length' : 30\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxZSF7rroGzp"
      },
      "source": [
        "## Blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p50Gn9cdoGzz"
      },
      "outputs": [],
      "source": [
        "# Layer normalization for regularizing the model\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, ndim, bias):\n",
        "    super().__init__()\n",
        "    self.weight, self.bias = nn.Parameter(torch.ones(ndim)), nn.Parameter(torch.zeros(ndim))\n",
        "\n",
        "  def forward(self, input):\n",
        "    return nn.functional.layer_norm(input=input, \n",
        "                                    normalized_shape=self.weight.shape, \n",
        "                                    weight=self.weight, \n",
        "                                    bias=self.bias, \n",
        "                                    eps=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlVCrSodoGzz"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.attention_layer = nn.Linear(config['n_embd'], 3 * config['n_embd'])\n",
        "    self.projection_layer = nn.Linear(config['n_embd'], config['n_embd'])\n",
        "    self.dropout = nn.Dropout(config['dropout'])\n",
        "\n",
        "  def attention_calculation (self, x):\n",
        "    query, key, value = self.attention_layer(x).split(config['n_embd'], dim=2)\n",
        "    key = key.view(x.size(0), x.size(1), config['n_head'], x.size(2) // config['n_head'])\n",
        "    query = query.view(x.size(0), x.size(1), config['n_head'], x.size(2) // config['n_head'])\n",
        "    value = value.view(x.size(0), x.size(1), config['n_head'], x.size(2) // config['n_head'])\n",
        "\n",
        "    key, query, value = key.transpose(1, 2), query.transpose(1, 2), value.transpose(1, 2)\n",
        "\n",
        "    y = nn.functional.scaled_dot_product_attention(query, key, value, dropout_p=config['dropout'])\n",
        "\n",
        "  def forward(self, x):\n",
        "    y = self.attention_calculation(x)\n",
        "    out = y.transpose(1, 2).view(x.size(0), x.size(1), x.size(2))\n",
        "    out = self.projection_layer(out)\n",
        "    out = self.dropout(out)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPbuhh7XoGzz"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config['n_embd'], bias=config['bias'])\n",
        "        self.attn = AttentionLayer()\n",
        "        self.ln_2 = LayerNorm(config['n_embd'], bias=config['bias'])\n",
        "        self.mlp = nn.Sequential (\n",
        "            nn.Linear(config['n_embd'], 4 * config['n_embd']),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config['n_embd'], config['n_embd']),\n",
        "            nn.Dropout(config['dropout'])\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EDtZ3PcoGzz"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Equl2QngoGzz"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embed = nn.Embedding(config['vocab_size'], config['n_embd'])\n",
        "    self.pos_embed = nn.Embedding(config['block_size'], config['n_embd'])\n",
        "    self.dropout = nn.Dropout(config['dropout'])\n",
        "\n",
        "    modules = [Block(config) for b in range(config['n_layer'])]\n",
        "    self.blocks = nn.Sequential(*modules)\n",
        "    self.layernorm = LayerNorm(config['n_embd'], bias=config['bias'])\n",
        "\n",
        "    self.lin1 = nn.Linear(config['n_embd'], config['vocab_size'])\n",
        "    self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "  def forward(self, idx):\n",
        "      position = torch.arange(0, idx.size(1))\n",
        "\n",
        "      tok_emb = self.token_embed(idx)\n",
        "      pos_emb = self.pos_embed(position)\n",
        "      x = self.dropout(tok_emb + pos_emb)\n",
        "\n",
        "      for block in self.blocks: x = block(x)\n",
        "      x = self.layernorm(x)\n",
        "\n",
        "      return self.lin1(x)\n",
        "\n",
        "  def generate(self, idx, max_new_tokens, temperature=1.0):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx if idx.size(1) <= self.config['block_size'] else idx[:, -self.config['block_size']:]\n",
        "        logits, _ = self(idx_cond)\n",
        "        probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlTHyR5ZB3Ym"
      },
      "source": [
        "## CNN DAILY MAIL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TI3NFBuGQvpQ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"cnn_dailymail\", '3.0.0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTmDPzZpvcUQ",
        "outputId": "98baa6f4-a6b8-4d63-d454-f04876a41740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['train', 'validation', 'test'])\n"
          ]
        }
      ],
      "source": [
        "print(dataset.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkuC9VJ7YNGN"
      },
      "outputs": [],
      "source": [
        "END_OF_TEXT = 50256\n",
        "START_OF_TEXT = 50255\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "summarization_prompt = enc.encode_ordinary(\"Summarize this article:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkkKffW0VYuh",
        "outputId": "85e4d826-8dd6-4893-dd7b-d82140e25017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[13065, 3876, 1096, 428, 2708, 25]\n"
          ]
        }
      ],
      "source": [
        "print(summarization_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQyh2CMJPLFD"
      },
      "outputs": [],
      "source": [
        "def split_into_chunks(encoded_article, article_index, chunk_size=250):\n",
        "    end_range = len(encoded_article) - len(encoded_article) % chunk_size\n",
        "    return [(summarization_prompt + encoded_article[i:i + chunk_size], article_index) for i in range(0, end_range, chunk_size)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BT6Lc3SsP4qg"
      },
      "outputs": [],
      "source": [
        "def pad_with_eos(text, length=config['summary_length']):\n",
        "  if length > len(text):\n",
        "    pad_len = length - len(text)\n",
        "    text += [END_OF_TEXT] * pad_len\n",
        "    return text\n",
        "  else:\n",
        "    return text[:length]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5qmyL2DJAT3"
      },
      "outputs": [],
      "source": [
        "def flatten_extend(matrix):\n",
        "    flat_list = []\n",
        "    for row in matrix:\n",
        "      flat_list.extend(row)\n",
        "    return flat_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5cDvJ8E_hfQ"
      },
      "outputs": [],
      "source": [
        "# Dataset class to load train and validation data\n",
        "\n",
        "class CNNDailyMailDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, prefix, encoder):\n",
        "\n",
        "        data = dataset[prefix]\n",
        "\n",
        "        self.enc = encoder\n",
        "\n",
        "        self.inputs = [split_into_chunks(self.enc.encode_ordinary(data[i][\"article\"]), i)\n",
        "            if (data[i][\"article\"] != None) else [] for i in range(len(data))]\n",
        "\n",
        "        self.inputs = flatten_extend(self.inputs)\n",
        "\n",
        "        self.targets = np.array([pad_with_eos(enc.encode_ordinary(data[i][\"highlights\"])) for i in range(len(data))])\n",
        "\n",
        "        self.length = len(self.inputs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        article_chunk = self.inputs[ind][0]\n",
        "        article_index = self.inputs[ind][1]\n",
        "        return torch.tensor(article_chunk), torch.tensor(self.targets[article_index]), article_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnKRglS4-l4Z"
      },
      "outputs": [],
      "source": [
        "class CNNDailyMailTestDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, prefix, encoder):\n",
        "\n",
        "        data = dataset[prefix]\n",
        "\n",
        "        self.enc = encoder\n",
        "\n",
        "        self.inputs = [split_into_chunks(self.enc.encode_ordinary(data[i][\"article\"]), i)\n",
        "            if (data[i][\"article\"] != None) else [] for i in range(len(data))]\n",
        "\n",
        "        self.targets = np.array([data[i]['highlights'] for i in range(len(data))])\n",
        "\n",
        "        self.length = len(self.inputs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        return self.inputs[ind], self.targets[ind]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIrqy1cDAzBX"
      },
      "outputs": [],
      "source": [
        "train_data = CNNDailyMailDataset(prefix=\"train\", encoder=enc)\n",
        "val_data = CNNDailyMailDataset(prefix=\"validation\",encoder=enc)\n",
        "test_data = CNNDailyMailTestDataset(prefix=\"test\",encoder=enc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_AgIb5izcqy",
        "outputId": "ce4796b7-aaa8-410f-e5e8-a552b14012bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n"
          ]
        }
      ],
      "source": [
        "print(len(list(train_data.__getitem__(3)[1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h8A-oEVA9Rw"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "     dataset     = train_data,\n",
        "     num_workers = 1,\n",
        "     batch_size  = config['batch_size'],\n",
        "     pin_memory  = True,\n",
        "     drop_last   = True,\n",
        "     shuffle     = True\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "     dataset     = val_data,\n",
        "     num_workers = 1,\n",
        "     batch_size  = config['batch_size'],\n",
        "     pin_memory  = True,\n",
        "     drop_last   = True,\n",
        "     shuffle     = False\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_data,\n",
        "    num_workers = 1,\n",
        "    batch_size  = 1,\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4er1tTL8BNzo",
        "outputId": "c74e4de8-9a96-42f5-9dbe-41052cd016e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 256]) torch.Size([64, 30])\n",
            "tensor([153355, 124372, 191885, 248379, 175910, 116445,  22201, 104562, 142908,\n",
            "         59902, 175479, 231670, 272493, 126000, 217487, 263987, 177404, 132113,\n",
            "        173588, 224715,  95684, 275608, 105229,  23827, 192049, 215502,  30395,\n",
            "        172546, 212987, 269283, 264306,  57238,  31544,  30474, 173563, 176491,\n",
            "         60742, 148873, 135951, 225883,  33081, 243449,  62388, 230082, 179230,\n",
            "        274306, 261521,  90175, 199905,  62483, 146215, 169031, 270411,  34565,\n",
            "        264669,  92047, 183236,  41192, 273252, 161119, 108714, 132067, 245069,\n",
            "        215919])\n",
            "tensor([[13065,  3876,  1096,  ..., 12526,  1683,  2826],\n",
            "        [13065,  3876,  1096,  ...,  9074, 26618,  2087],\n",
            "        [13065,  3876,  1096,  ...,  2863,    11,   772],\n",
            "        ...,\n",
            "        [13065,  3876,  1096,  ...,  2263,  5986,   286],\n",
            "        [13065,  3876,  1096,  ...,  1521,   326, 30597],\n",
            "        [13065,  3876,  1096,  ...,    11,   705,  7109]]) tensor([[31407,   318,   517,  ...,   284,   711,  8685],\n",
            "        [18308, 26618,   373,  ...,  2237,    12,  8589],\n",
            "        [19962, 15868,   979,  ..., 10033,  4302,  6075],\n",
            "        ...,\n",
            "        [ 5167,   621,  7337,  ...,  1542,  2693,   287],\n",
            "        [25631, 33508,  8565,  ...,  8837,   468,   300],\n",
            "        [   50, 16809,    75,  ...,   286,  1719,  1813]])\n"
          ]
        }
      ],
      "source": [
        "# Testing code to check if your data loaders are working\n",
        "for i, data in enumerate(train_loader):\n",
        "     x, y, article_idx = data\n",
        "     print(x.shape, y.shape)\n",
        "     print(article_idx)\n",
        "     print(x, y)\n",
        "     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If2zw_RvzwHC",
        "outputId": "cad886ac-61a1-4b60-8253-9a9475cd1c0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0])\n",
            "['Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\\nIsrael and the United States opposed the move, which could open the door to war crimes investigations against Israelis .']\n"
          ]
        }
      ],
      "source": [
        "for i, data in enumerate(test_loader):\n",
        "  x, y = data\n",
        "  print(x[1][1])\n",
        "  print(y)\n",
        "  break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rlmmrv3CWy09",
        "outputId": "6fa96462-8cbd-4e4a-994f-13e03f91a6d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# prompt: Instantiate and load a model from a checkpoint file\n",
        "\n",
        "model = GPT(config).to(device)\n",
        "checkpoint_path = './pretrained_model_checkpoint.pth'\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "model.load_state_dict(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tFDfPECtCO6"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=END_OF_TEXT)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr= config['init_lr']) #Defining Optimizer\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.75)\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1TCHUMdkdZb"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKvcyzUgAven"
      },
      "outputs": [],
      "source": [
        "def generate_text_logits(model, seq, max_new_tokens=config['summary_length']):\n",
        "  text_logits = None\n",
        "\n",
        "  for i in range(max_new_tokens):\n",
        "      # if the sequence context is growing too long we must crop it at block_size\n",
        "      seq_cond = seq if seq.size(1) <= config['block_size'] else seq[:, -config['block_size']:]\n",
        "      # forward the model to get the logits for the index in the sequence\n",
        "      logits = model(seq_cond)\n",
        "\n",
        "      if i == max_new_tokens-1:\n",
        "        text_logits = logits[:, -max_new_tokens:, :] # batch size, max_new_tokens, vocab_size\n",
        "\n",
        "      # pluck the logits at the final step and scale by desired temperature\n",
        "      logits = logits[:, -1, :]\n",
        "      # apply softmax to convert logits to (normalized) probabilities\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      # append sampled index to the running sequence and continue\n",
        "      seq = torch.cat((seq, idx_next), dim=1)\n",
        "\n",
        "  return text_logits, seq[:, -max_new_tokens:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYJPadjWlHoW"
      },
      "outputs": [],
      "source": [
        "def calculate_bleu_score(reference, candidate):\n",
        "    \"\"\"\n",
        "    Calculate the average BLEU score for a batch of translations.\n",
        "\n",
        "    Args:\n",
        "    - references_batch: A list of lists, where each inner list contains a single reference translation.\n",
        "    - candidates_batch: A list of candidate translations.\n",
        "\n",
        "    Returns:\n",
        "    - average_bleu_score: The average BLEU score for the entire batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the strings\n",
        "    reference_tokenized = nltk.word_tokenize(reference)\n",
        "    candidate_tokenized = nltk.word_tokenize(candidate)\n",
        "\n",
        "    # Calculate BLEU score for each translation in the batch\n",
        "    bleu_score = corpus_bleu([reference_tokenized], [candidate_tokenized])\n",
        "\n",
        "    return bleu_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EynfjERYLMe"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "\n",
        "    model.train()\n",
        "    tloss = 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    counter = 0\n",
        "\n",
        "    for i, (inputs, targets, article_idx) in enumerate(dataloader):\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "        ### Initialize Gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "          ### Move Data to Device (Ideally GPU)\n",
        "          inputs      = inputs.to(device)\n",
        "          targets    = targets.to(device)\n",
        "\n",
        "          ### Forward Propagation\n",
        "          logits, _ = generate_text_logits(model, inputs)\n",
        "\n",
        "          B, T, C = logits.shape\n",
        "\n",
        "          logits = logits.reshape(B*T, -1)\n",
        "          targets = targets.reshape(-1)\n",
        "\n",
        "          loss =  criterion(logits, targets)\n",
        "\n",
        "\n",
        "        ### Backward Propagation\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        ### Gradient Descent\n",
        "        scaler.step(optimizer)\n",
        "\n",
        "        scaler.update()\n",
        "\n",
        "        tloss   += loss.item()\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        if counter % 50 == 0:\n",
        "          wandb.log({'train_loss': (tloss/counter), 'lr': curr_lr})\n",
        "\n",
        "\n",
        "        ### Release memory\n",
        "        del inputs, targets, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    tloss   /= len(dataloader)\n",
        "\n",
        "    return tloss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twhD90IsYjQ2"
      },
      "outputs": [],
      "source": [
        "def eval(model, dataloader):\n",
        "\n",
        "    model.eval() # set model in evaluation mode\n",
        "    vloss = 0 # Monitoring loss, accuracy, and distance\n",
        "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(dataloader):\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "        ### Move data to device (ideally GPU)\n",
        "        inputs      = inputs.to(device)\n",
        "        targets    = targets.to(device)\n",
        "\n",
        "        # makes sure that there are no gradients computed as we are not training the model now\n",
        "        with torch.inference_mode():\n",
        "            logits, _ = generate_text_logits(model, inputs)\n",
        "\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits = logits.reshape(B*T, -1)\n",
        "            targets = targets.reshape(-1)\n",
        "\n",
        "            loss =  criterion(logits, targets)\n",
        "\n",
        "\n",
        "        # strip\n",
        "        vloss   += loss.item()\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.07f}\".format(float(vloss / (i + 1))))\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "        if counter % 50 == 0:\n",
        "          wandb.log({'val_loss': (vloss/counter), 'dist/lr': curr_lr})\n",
        "\n",
        "        ### Release memory\n",
        "        del inputs, targets, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    vloss   /= len(dataloader)\n",
        "\n",
        "    return vloss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMivH6Uuqpcx"
      },
      "outputs": [],
      "source": [
        "def test(model, dataloader):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n",
        "\n",
        "  counter = 0\n",
        "\n",
        "  bleu_score = 0.0\n",
        "\n",
        "  for i, (inputs, targets) in enumerate(dataloader):\n",
        "\n",
        "    counter += 1\n",
        "\n",
        "    generated_summary = []\n",
        "\n",
        "    for chunk in inputs:\n",
        "      chunk = torch.tensor([chunk[0]]).to(device)\n",
        "      _ , text = generate_text_logits(model, chunk)\n",
        "      truncated_text = []\n",
        "      for token in text[0]:\n",
        "        if token == END_OF_TEXT:\n",
        "          break\n",
        "        truncated_text.append(token)\n",
        "      generated_summary.extend(truncated_text)\n",
        "\n",
        "    generated_summary = enc.decode(generated_summary)\n",
        "\n",
        "    bleu_score += calculate_bleu_score(targets, generated_summary)\n",
        "\n",
        "    batch_bar.set_postfix(loss=\"{:.07f}\".format(float(bleu_score / (i + 1))))\n",
        "\n",
        "\n",
        "  batch_bar.close()\n",
        "  bleu_score /= len(dataloader)\n",
        "\n",
        "  return bleu_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgoLXmGTNiiW",
        "outputId": "b09c842d-7208-42c5-f8cb-9eb24d459323"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkkmittal\u001b[0m (\u001b[33midl-f23\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login(key=\"9312acc23a6389a925ba54b1bdf81ff99fe4d2e4\") # API key for the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JDo6TRsNpTZ",
        "outputId": "c60c8a3e-89c7-4abd-edd0-a5295996d139"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231211_215308-7t817bz5</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/idl-f23/hw5-finetune/runs/7t817bz5' target=\"_blank\">summarization-finetuning</a></strong> to <a href='https://wandb.ai/idl-f23/hw5-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/idl-f23/hw5-finetune' target=\"_blank\">https://wandb.ai/idl-f23/hw5-finetune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/idl-f23/hw5-finetune/runs/7t817bz5' target=\"_blank\">https://wandb.ai/idl-f23/hw5-finetune/runs/7t817bz5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create your wandb run\n",
        "run = wandb.init(\n",
        "    name    = \"summarization-finetuning\", ### Wandb last name initializer\n",
        "    reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    project = \"hw5-finetune\", ### Project should be created in WandB\n",
        "    config  = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Sgb4lw6Ux3ig",
        "outputId": "e80ec426-ced3-433c-8217-beea92016c6b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTest:   0%|          | 0/11490 [00:00<?, ?it/s]"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-bcda8ee31405>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbleu_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-65-c89a04026c09>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mgenerated_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mbleu_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcalculate_bleu_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mbatch_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{:.07f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbleu_score\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-5a78bb744e0f>\u001b[0m in \u001b[0;36mcalculate_bleu_score\u001b[0;34m(reference, candidate)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Tokenize the strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mreference_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mcandidate_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "bleu_score = test(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlStyodfkXdy",
        "outputId": "8dd47404-1caf-4447-f52b-fad65bc11576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 0/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train:  20%|█▉        | 2657/13345 [4:41:51<18:54:31,  6.37s/it, loss=7.0129]"
          ]
        }
      ],
      "source": [
        "# Iterate over number of epochs to train and evaluate your model\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch, config['epochs']))\n",
        "\n",
        "    curr_lr      = float(optimizer.param_groups[0]['lr'])\n",
        "    train_loss   = train(model, train_loader, optimizer, criterion)\n",
        "    val_loss     = eval(model, val_loader)\n",
        "\n",
        "    print(\"\\tTrain Loss {:.07f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
        "    print(\"\\tVal Loss {:.07f}\\t\".format(val_loss))\n",
        "\n",
        "    ### Log metrics at each epoch in your run\n",
        "    # Optionally, you can log at each batch inside train/eval functions\n",
        "    # (explore wandb documentation/wandb recitation)\n",
        "    # wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n",
        "    #            'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})\n",
        "    wandb.log({'train_loss': train_loss, 'valid_loss': val_loss, 'lr': curr_lr})\n",
        "\n",
        "    ### Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best\n",
        "    torch.save({'model_state_dict' : model.state_dict(), 'optimizer_state_dict' : optimizer.state_dict()}, '/content/Finetuning/Summarization/checkpoint_epoch_' + str(epoch) + '.pth')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      torch.save({'model_state_dict' : model.state_dict(), 'optimizer_state_dict' : optimizer.state_dict()}, '/content/Finetuning/Summarization/checkpoint_best.pth')\n",
        "\n",
        "    scheduler.step(val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U9wZd0p2P_X"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
